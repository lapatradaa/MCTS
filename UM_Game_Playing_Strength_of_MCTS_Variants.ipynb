{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapatradaa/MCTS/blob/main/UM_Game_Playing_Strength_of_MCTS_Variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vYBpKvDFk3l1"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7wAvxtyUK5-p",
        "outputId": "f4e545b5-400f-459b-e9d5-8d819e5a7f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         )\n\u001b[0;32m--> 287\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/MCTS/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/MCTS/test.csv')\n",
        "concepts = pd.read_csv('/content/drive/MyDrive/MCTS/concepts.csv')"
      ],
      "metadata": {
        "id": "QzWYcjwylcu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(5)"
      ],
      "metadata": {
        "id": "L6efpYjaNIhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop all columns with no unique values\n",
        "\n",
        "for column in train.columns:\n",
        "  if train[column].nunique() == 1:\n",
        "    train = train.drop(column, axis=1)\n"
      ],
      "metadata": {
        "id": "Smip0k4inERq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop all columns where nunique = 0\n",
        "\n",
        "for column in train.columns:\n",
        "  if train[column].nunique() == 0:\n",
        "    train = train.drop(column, axis=1)\n",
        "\n",
        "print(f\"Number of columns: {len(train.columns)}\")\n",
        "for column in train.columns:\n",
        "  print(f\"{column}: {train[column].nunique()}\")\n"
      ],
      "metadata": {
        "id": "lWgmV34koJ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop Id column\n",
        "\n",
        "train = train.drop('Id', axis=1)\n"
      ],
      "metadata": {
        "id": "LdBAnLVUpquv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the number of columns and the number of unqiue values of each column\n",
        "\n",
        "print(f\"Number of columns: {len(train.columns)}\")\n",
        "for column in train.columns:\n",
        "  print(f\"{column}: {train[column].nunique()}\")\n"
      ],
      "metadata": {
        "id": "FxiokiE-nKxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "TF-kmPEQHy2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Modeling\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Saving the model\n",
        "import joblib\n",
        "\n",
        "# For handling warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "y8hLyIzACE_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the training data\n",
        "print(\"Missing Values in Training Data:\")\n",
        "print(train.isnull().sum().sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "FAfUFejePJxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numeric_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_features.remove('utility_agent1')  # Exclude target variable\n",
        "\n",
        "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumber of numerical features: {len(numeric_features)}\")\n",
        "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# Initialize imputers\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Impute numerical features\n",
        "train[numeric_features] = imputer_num.fit_transform(train[numeric_features])\n",
        "\n",
        "# Impute categorical features\n",
        "train[categorical_features] = imputer_cat.fit_transform(train[categorical_features])\n"
      ],
      "metadata": {
        "id": "5N3mwhSCQaGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse agent strings\n",
        "def parse_agent(agent_str):\n",
        "    try:\n",
        "        parts = agent_str.split('-')\n",
        "        return pd.Series({\n",
        "            'SELECTION': parts[1],\n",
        "            'EXPLORATION_CONST': float(parts[2]),\n",
        "            'PLAYOUT': parts[3],\n",
        "            'SCORE_BOUNDS': 1 if parts[4].lower() == 'true' else 0\n",
        "        })\n",
        "    except:\n",
        "        # Handle unexpected formats by assigning default or unknown values\n",
        "        return pd.Series({\n",
        "            'SELECTION': 'Unknown',\n",
        "            'EXPLORATION_CONST': np.nan,\n",
        "            'PLAYOUT': 'Unknown',\n",
        "            'SCORE_BOUNDS': 0\n",
        "        })\n",
        "\n",
        "# Parse 'agent1'\n",
        "agent1_parsed = train['agent1'].apply(parse_agent)\n",
        "agent1_parsed = agent1_parsed.rename(columns=lambda x: f'agent1_{x}')\n",
        "train = pd.concat([train, agent1_parsed], axis=1)\n",
        "\n",
        "# Parse 'agent2'\n",
        "agent2_parsed = train['agent2'].apply(parse_agent)\n",
        "agent2_parsed = agent2_parsed.rename(columns=lambda x: f'agent2_{x}')\n",
        "train = pd.concat([train, agent2_parsed], axis=1)\n",
        "\n",
        "# Drop the original 'agent1' and 'agent2' columns\n",
        "train.drop(['agent1', 'agent2'], axis=1, inplace=True)\n",
        "\n",
        "# Check for any missing values introduced by parsing\n",
        "print(\"\\nMissing Values in Parsed Agent Features:\")\n",
        "print(train[['agent1_EXPLORATION_CONST', 'agent2_EXPLORATION_CONST']].isnull().sum())\n",
        "\n",
        "# Impute any missing 'EXPLORATION_CONST' with median\n",
        "train[['agent1_EXPLORATION_CONST', 'agent2_EXPLORATION_CONST']] = imputer_num.fit_transform(\n",
        "    train[['agent1_EXPLORATION_CONST', 'agent2_EXPLORATION_CONST']]\n",
        ")\n"
      ],
      "metadata": {
        "id": "8qysm_8EghuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify remaining categorical features after parsing agents\n",
        "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nCategorical Features to Encode: {categorical_features}\")\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "train = pd.get_dummies(train, columns=categorical_features, drop_first=True)\n",
        "\n",
        "print(f\"\\nTraining Data Shape after One-Hot Encoding: {train.shape}\")\n"
      ],
      "metadata": {
        "id": "vHLaaS0sg1M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical features (excluding the target)\n",
        "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numerical_features.remove('utility_agent1')\n",
        "\n",
        "print(f\"\\nNumerical Features to Scale: {numerical_features}\")\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform on training data\n",
        "train[numerical_features] = scaler.fit_transform(train[numerical_features])\n",
        "\n",
        "print(\"\\nNumerical Features have been scaled.\")\n"
      ],
      "metadata": {
        "id": "tTJsvUvlhKaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Initialize VarianceThreshold with a threshold (e.g., 0.01)\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "\n",
        "# Fit the selector on the training data (excluding target)\n",
        "selector.fit(train.drop(['utility_agent1'], axis=1))\n",
        "\n",
        "# Get the list of features to keep\n",
        "features_to_keep = train.drop(['utility_agent1'], axis=1).columns[selector.get_support()]\n",
        "\n",
        "print(f\"\\nNumber of features before Variance Threshold: {train.shape[1]-1}\")\n",
        "print(f\"Number of features after Variance Threshold: {len(features_to_keep)}\")\n",
        "\n",
        "# Reduce the dataset to only keep these features\n",
        "train = train[features_to_keep.tolist() + ['utility_agent1']]\n"
      ],
      "metadata": {
        "id": "xedMNsiihMzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix\n",
        "corr_matrix = train.drop(['utility_agent1'], axis=1).corr().abs()\n",
        "\n",
        "# Select the upper triangle of the correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "print(f\"\\nNumber of features to drop due to high correlation: {len(to_drop)}\")\n",
        "\n",
        "# Drop these highly correlated features\n",
        "train.drop(to_drop, axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "m8mClESThW0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = train.drop(['utility_agent1'], axis=1)\n",
        "y = train['utility_agent1']\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "importances = importances.sort_values(ascending=False)\n",
        "\n",
        "# Plot the top 20 feature importances\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=importances[:20], y=importances.index[:20])\n",
        "plt.title('Top 20 Feature Importances')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Select top N features (e.g., top 100)\n",
        "top_n = 100\n",
        "top_features = importances[:top_n].index.tolist()\n",
        "\n",
        "print(f\"\\nSelecting Top {top_n} Features based on Importance.\")\n",
        "\n",
        "# Reduce the dataset to only keep top features\n",
        "train = train[top_features + ['utility_agent1']]\n"
      ],
      "metadata": {
        "id": "9p_izcFWhYbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target\n",
        "X = train.drop(['utility_agent1'], axis=1)\n",
        "y = train['utility_agent1']\n",
        "\n",
        "# Split the data (80% training, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining Set Size: {X_train.shape}\")\n",
        "print(f\"Validation Set Size: {X_val.shape}\")\n"
      ],
      "metadata": {
        "id": "Y-rP4jrvhZ5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nRandom Forest model has been trained.\")"
      ],
      "metadata": {
        "id": "ZwsLqGyKhl3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = rf.predict(X_val)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "print(f\"\\nRandom Forest RMSE on Validation Set: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "ARyPx7SyhmpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_val, y_pred, alpha=0.3)\n",
        "plt.xlabel('Actual Utility Agent1')\n",
        "plt.ylabel('Predicted Utility Agent1')\n",
        "plt.title('Actual vs. Predicted Utility Agent1')\n",
        "plt.plot([-1,1], [-1,1], color='red')  # Diagonal line for reference\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tq_qor1rhnfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_val - y_pred\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(residuals, bins=50, kde=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CpF7X4FBhoXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the trained model\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
        "feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "# Plot the top 20 important features\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=feature_importances[:20], y=feature_importances.index[:20])\n",
        "plt.title('Top 20 Feature Importances in Random Forest')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5N8CNVB5hpID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "print(\"\\nStarting Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"\\nBest Parameters from Grid Search: {best_params}\")\n",
        "\n",
        "# Best score (negative MSE)\n",
        "best_score = grid_search.best_score_\n",
        "best_rmse = np.sqrt(-best_score)\n",
        "print(f\"Best CV RMSE from Grid Search: {best_rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "TK1vZQfQhp9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define parameter distributions\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(10, 50),\n",
        "    'min_samples_split': randint(2, 11),\n",
        "    'min_samples_leaf': randint(1, 5)\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV on the training data\n",
        "print(\"\\nStarting Randomized Search...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params_random = random_search.best_params_\n",
        "print(f\"\\nBest Parameters from Randomized Search: {best_params_random}\")\n",
        "\n",
        "# Best score (negative MSE)\n",
        "best_score_random = random_search.best_score_\n",
        "best_rmse_random = np.sqrt(-best_score_random)\n",
        "print(f\"Best CV RMSE from Randomized Search: {best_rmse_random:.4f}\")\n"
      ],
      "metadata": {
        "id": "m6iWR6wbhrBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the final Random Forest Regressor with best hyperparameters\n",
        "final_rf = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the final model on the entire dataset\n",
        "final_rf.fit(X, y)\n",
        "\n",
        "print(\"\\nFinal Random Forest model has been trained on the entire dataset.\")\n"
      ],
      "metadata": {
        "id": "3B8AgFi0hsLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(\n",
        "    final_rf, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate RMSE for each fold\n",
        "cv_rmse = np.sqrt(-cv_scores)\n",
        "print(f\"\\nCross-Validation RMSE: {cv_rmse.mean():.4f} Â± {cv_rmse.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "a_zV_8xFhtcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the final model\n",
        "feature_importances_final = pd.Series(final_rf.feature_importances_, index=X.columns)\n",
        "feature_importances_final = feature_importances_final.sort_values(ascending=False)\n",
        "\n",
        "# Plot the top 20 feature importances\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=feature_importances_final[:20], y=feature_importances_final.index[:20])\n",
        "plt.title('Top 20 Feature Importances in Final Random Forest Model')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdnWQf7LhvDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGw9kgAUhxCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}